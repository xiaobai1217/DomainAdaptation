<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Audio-Adaptive Activity Recognition Across Video Domains</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Audio-Adaptive Activity Recognition Across Video Domains" />
	<meta property="og:description" content="CVPR 2022" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Audio-Adaptive Activity Recognition Across Video Domains</span>
		<p> </p>
		<table align=center width=800px>
			<table align=center width=800px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://xiaobai1217.github.io">Yunhua Zhang<sup>1</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://hazeldoughty.github.io/">Hazel Doughty<sup>1</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="">Ling Shao<sup>2</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://www.ceessnoek.info/">Cees G.M. Snoek<sup>1</a></span>
						</center>
					</td>
				</tr>
			</table>
			<p> </p>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><sup>1</sup> <a href="https://ivi.fnwi.uva.nl/vislab/">VIS Lab</a>, University of Amsterdam</span>
						</center>
					</td>
					<td align=center width=400px>
						<center>
							<span style="font-size:24px"><sup>2</sup> Inception Institute of Artificial Intelligence</a></span>
						</center>
					</td>

				</tr>
			</table>
			<p> </p>
			<table align=center width=640px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/xiaobai1217/DomainAdaptation'>[Code]</a></span><br>
						</center>
					</td>
					<td align=center width=220px>
						<center>
							<span style="font-size:24px"><a href='https://drive.google.com/file/d/11ubcWqu1CiHwXBrfM9Ln5w0Y1Bz6kyq4/view?usp=sharing'>[ActorShift Dataset]</a></span><br>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href='https://drive.google.com/file/d/1EuG4n-aunOzCmb3gBsTuJSpY8B0Y40L3/view?usp=sharing'>[Demo Video]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<p> </p>



	<table align=center width=1100px>
  			  <tr>
 	              <td width=370px>
  					<center>
  	                	<img src = "./resources/1st-figure.png" height="520px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=1100px>
					<tr>
		              <td align=center width=1100px>
		              <span style="font-size:15px"><b>We recognize activities under domain shifts, caused by change of scenery, camera viewpoint or actor, with the aid of sound. </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>



	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				This paper strives for activity recognition under domain shift, for example caused by change of scenery or camera viewpoint. The leading approaches reduce the shift in activity appearance by adversarial training and self-supervised learning. Different from these vision-focused works we leverage activity sounds for domain adaptation as they have less variance across domains and can reliably indicate which activities are not happening. We propose an audio-adaptive encoder and associated learning methods that discriminatively adjust the visual feature representation as well as addressing shifts in the semantic distribution. To further eliminate domain-specific features and include domain-invariant activity sounds for recognition, an audio-infused recognizer is proposed,  which effectively models the cross-modal interaction across domains. We also introduce the new task of actor shift, with a corresponding audio-visual dataset, to challenge our method with situations where the activity appearance changes dramatically. Experiments on this dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach. For instance, we achieve a 5% absolute improvement over previous works in EPIC-Kitchens.
			</td>
		</tr>
	</table>
	<br>


	<center><h1>Method</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We train our audio-adaptive model in two stages using videos from source and target domains with accompanying audio. In the first stage we train our <b>audio-adaptive encoder</b> that uses audio to adapt a visual encoder to be more robust to distribution shifts. In the second stage, we train our <b>audio-infused recognizer</b> using pseudo-labels from the audio-adaptive encoder for the target domain and the ground-truth labels for the source domain. The audio-infused recognizer maps the source and target domains into a common space and fuses audio and visual features to produce an activity prediction for either domain

				</td>
			</tr>
		</center>
	</table>


	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/encoder.png" height="350px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Audio-adaptive encoder for activity recognition under domain shift </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>

						


	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our <b>audio-adaptive encoder</b> consists of a visual encoder, an audio encoder and an audio-based attention module. Since the sounds of activities have less variance across domains, the encoder aims to extract visual features that are invariant but discriminative under domain shift with the aid of the audio encoder pretrained for audio-based activity recognition. To this end, we train the visual encoder and the attention module with two audio-adaptive learning methods: absent-activity learning or unlabeled target data and audio-balanced learning for labeled source data. The former aims to remove irrelevant parts of the visual features while the latter helps to handle the differing label distribution between domains. Once trained, for each video, we can extract an audio feature vector from the audio encoder and a series of visual features from the visual encoder with which to train our audio-infused recognizer 

				</td>
			</tr>
		</center>
	</table>


		<table align=center width=1100px>
  			  <tr>
 	              <td width=370px>
  					<center>
  	                	<img src = "./resources/transformer.png" height="300px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=1100px>
					<tr>
		              <td align=center width=1100px>
		              <span style="font-size:15px"><b>Audio-infused recognizer. </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>


	<table align=center width=850px>
		<center>
			<tr>
				<td>
					While audio can help focus on the activity-relevant visual features inside our audio-adaptive encoder, there is still a large difference between the appearance of activities in different domains. To further eliminate domain-specific visual features and fuse the activity cues from the audio and visual modalities, we propose the <b>audio-infused recognizer</b>. As shown in the above figure, we add domain embedding to encourage a common visual representation across domains. Then, an audio-adaptive class token is obtained from a series of activity sound feature vectors, considering both audio and visual features. It is sent into the transformer together with the visual features. By the transformer’s self attention, this token aggregates information from visual features with the domain-invariant audio activity cues for activity classification.

				</td>
			</tr>
		</center>
	</table>

	<table align=center width=850px>
		<center><h1>ActorShift Dataset</h1></center>
		<tr>
			<td>
				We introduce ActorShift, where the domain shift comes from the change in actor species: we use humans in the source domain and animals in the target domain. This causes large variances in the appearance and motion of activities. For the corresponding dataset we select 1,305 videos of 7 human activity classes from Kinetics-700 as the source domain: sleeping, watching tv, eating, drinking, swimming, running and opening a door. For the target domain we collect 200 videos from YouTube of animals performing the same activities. We divide them into 35 videos for training (5 per class) and 165 for evaluation. The target domain data is scarce, meaning there is the additional challenge of adapting to the target domain with few unlabeled examples.
			</td>
		</tr>
	</table>
	<br>

		 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/actorshift.png" height="350px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Examples in our ActorShift dataset </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>
  		  <br>

	 <table align=center width=1100px>
	  		 	<center><h1>Demo Video</h1></center>
  			  <tr>
<!--   	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/teaser_v3.jpg" height="275px"></img>
  	                	<br>
					</center>
  	              </td> -->
		  		  <table align=center width=800px>
		  		    <tr>
		              <td align=center width=800px>
						<iframe width="800" height="450" src="https://youtube.com/embed/Lh3gb6NMhB4" frameborder="0" allowfullscreen></iframe>
					  </td>
				  </table>
				<br>
                </tr>
  		  </table>


	<br>
	<hr>
	<table align=center width=800px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Yunhua Zhang, Hazel Doughty, Ling Shao, Cees G.M. Snoek<br>
				<b>Audio-Adaptive Activity Recognition Across Video Domains</b><br>
				In CVPR, 2022.<br>
				(hosted on <a href="">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:14pt"><a href="./resources/bibtex.txt">[Bibtex]<br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<table align=center width=600px>
							<td align=center width=400px>
						<center>
							<span style="font-size:24px">Contact</span>
						</center>
					</td>

						<td align=center width=400px>
						<center>
							<span style="font-size:24px"><a href='mailto: y.zhang9@uva.nl'>[Email]</a></span>
						</center>
					</td>

					<td align=center width=400px>
						<center>
							<span style="font-size:24px"><a href='https://twitter.com/yunhua_zhang'>[Twitter]</a></span>
						</center>
					</td>
	</table>

	<hr>
	<br>

	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

