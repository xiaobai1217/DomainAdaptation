# Audio-Adaptive Activity Recognition Across Video Domains

## EPIC-Kitchens
* Prepare the audio files (.wav) from the videos:

```
python generate_sound_files.py
```

* Environments:

```
PyTorch 1.7.0
mmcv-full 1.2.7
mmaction2 0.13.0
cudatoolkit 10.1.243
```

### RGB and audio
**This is the demo code for training the audio-adaptive model with RGB and audio modalities on EPIC-Kitchens dataset, reproducing an mean accuracy of 59.2%.**

* First download the data following the code provided by an existing work https://github.com/jonmun/MM-SADA-code
* Go to the sub-directory
```
cd EPIC-rgb-audio
```


* To run the code on 4 NVIDIA 1080Ti GPUs:
```
sh bash.sh
```

### Optical flow and audio
Note that the clusters and absent-pseudo labels generated by audio are the same as those in the "RGB and audio" code
* Go to the sub-directory
```
cd EPIC-flow-audio
```


* To run the code on 4 NVIDIA 1080Ti GPUs:
```
sh bash.sh
```

## CharadesEgo

This code conducts semi-supervised domain adaptation with all the source (3rd-person view) data and half of the target (1st-person view) data, based on RGB and audio modalities. 
* Go to the sub-directory
```
cd CharadesEgo
```


* To run the code on 4 NVIDIA 1080Ti GPUs:
```
sh bash.sh
```


## ActorShift Dataset

This dataset can be downloaded at https://uvaauas.figshare.com/articles/dataset/ActorShift_zip/19387046
